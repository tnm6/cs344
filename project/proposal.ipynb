{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Proposal\n",
    "\n",
    "By Nathan Meyer for CS-344: Artifical Intelligence by Professor Keith VanderLinden, Calvin University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update\n",
    "\n",
    "My project proposal involves identifying a film's genre based upon its dialogue. Using [this dataset](http://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) and basing the implementation off of our class' IMDb movie reviews exercise, the formulation would involve a neural network with categorical classification of the probability of which movie genre(s) the film is in based upon its dialogue.\n",
    "\n",
    "The dataset itself includes information about the films included in the study, information about characters, the individual lines spoken by characters in given movies (using IDs), and the conversations which contain those ID'd lines. Each sub-dataset is formatted in .tsv and contains proper IDs to link to other files. There is also an [SQLite conversion of this dataset](https://www.kaggle.com/mrlarichards/cornell-movie-dialogs-corpus-sqlite/).\n",
    "\n",
    "My aim is to be able to, like in the reviews exercise, gather the number of occurrences of words for each film's dialog to train the model on which commonly-occurring words might indicate its genre(s). I expect that a significant portion of the work will involve formulating the dataset in a manner which be Keras/TensorFlow friendly. To do this, I plan to gather the data relevant to this project using Pandas' data processing tools (read_csv, etc.) based on [examples like these](https://www.kaggle.com/ksjpswaroop/movie-dialog-generation-keras-model). The corpus' files include entries separated by a consistent token and each file is connected through film and line ids, so I plan to create dataframes where the data entries are the movies (titles might be included but not used as a feature since they could be too easy), with the list of tokenized (using Keras' tokenizer) words for the film's dialog, then one-hot-encoded. The labels will be the genres for each respective movie, possibly also indexed since many films have multiple genres. In case of any ordering, I plan to shuffle the entirety of the data before splitting them into training and testing datasets; since there are only 615 films, there might be too few entries to include an additional validation set.\n",
    "\n",
    "With these datasets, I plan for this to involve a densely-connected neural network with the one-hot-encoded tensors. Much of the initial set-up for this will be based upon the Chollett IMDb exercise, as well as the initial architecture of the network itself. However, the output will need to include more than one node since this is categorical classification, accounting for multiple genres. If possible, this output should account for where a film might be of multiple genres; however, problems with the data included multiple genres for the film might arise and require the data to be pruned based upon problematic or outlier instances (overly-common or overly-rare genres)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}