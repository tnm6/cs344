{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Updates\n",
    "\n",
    "Updates since project walkthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refactored Data Collection\n",
    "\n",
    "The code to collect the Keras datasets has been refactored into a separate file, which is imported like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cornell import Cornell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And loaded in like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Cornell()\n",
    "(train_data, train_labels), (test_data, test_labels) = dataset.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, loading in the data takes a minute or two, particularly the section where it collects the dialog strings for each movie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class vs. Functional Module\n",
    "\n",
    "The decision to make the dataset based around was a class was so that the project submission could easily and quickly retrieve the genres index in order to report on some example predictions in \"human-readable\" form. The index is first saved as a dictionary, translating strings to encoded indices (for transforming within the dataset load), then saved back into a list with each index representing the corresponding index in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode each genre name into an integer and link them via dict\n",
    "genre_ints = encoder.fit_transform(genre_names)\n",
    "for i in range(len(genre_names)):\n",
    "    self.genre_to_idx[genre_names[i]] = genre_ints[i]\n",
    "\n",
    "# Create a conversion table for index back to genre name\n",
    "self.idx_to_genre = [''] * len(genre_names)\n",
    "for genre in genre_names:\n",
    "    self.idx_to_genre[self.genre_to_idx[genre]] = genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Simple getters are thus implemented to retrieve these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation with Model Configurations\n",
    "\n",
    "As explained within the report, the initial model based upon Chollet's IMDb reviews exercise yielded notable results from the get-go. Only a few alterations were made in order to push the accuracy to around 90.5% on the test set. No further alterations yielded higher results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(num_genres, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=16, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some extra code\n",
    "\n",
    "To ensure that the built-in Keras evaluation function was functioning properly, I also wrote a simple set of code to check by hand that the accuracy data was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)\n",
    "predictions[predictions>=0.5] = 1\n",
    "predictions[predictions<0.5] = 0\n",
    "\n",
    "entries = len(y_test)\n",
    "total = num_genres * entries\n",
    "\n",
    "correct = 0\n",
    "for i in range(entries):\n",
    "    for j in range(num_genres):\n",
    "        correct += int(predictions[i][j] == y_test[i][j])\n",
    "\n",
    "print(str(correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This, as seen in the report, prints out the same accuracy percentage as evaluate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(idx):\n",
    "    predicted = \"\"\n",
    "    actual = \"\"\n",
    "    for i in range(num_genres):\n",
    "        if predictions[idx][i] > 0:\n",
    "            predicted += idx_to_genre[i] + ' '\n",
    "        if y_test[idx][i] > 0:\n",
    "            actual += idx_to_genre[i] + ' '\n",
    "    print(\"Predicted: \" + predicted)\n",
    "    print(\"Actual: \" + actual)\n",
    "\n",
    "for i in range(20):\n",
    "    print_results(i)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this shows a few examples translated back into readable genres."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}