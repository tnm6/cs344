{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Movie Genre From Only Its Dialog\n",
    "\n",
    "For CS-344: Artificial Intelligence by Professor Keith VanderLinden, Calvin University.\n",
    "\n",
    "Project and report by Nathan Meyer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vision\n",
    "\n",
    "This project is an experimentation with machine learning, neural networks specifically, to classify movies by genre through only the movie's dialog text. The project was envisioned as a multi-class, multi-label extension of the [IMDb reviews classification exercise](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/3.5-classifying-movie-reviews.ipynb) from F. Chollet's Deep Learning with Python textbook, expanding that exercise's binary classification model into one that not only could classify categorically, but could classify entries with multiple labels per entry.\n",
    "\n",
    "The inspiration for this project came from finding the [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html), which was originally conceived as a part of another machine learning research project. Given the work that had already been performed on this dataset to categorize movie lines by which movie they were in, it seemed feasible to convert this data into something I could use for this project.\n",
    "\n",
    "Since this dataset includes movies, its (zero or more) genres, and all of the dialog lines contained within each movie, I could theoretically picture a translation of the IMDb classification exercise. Instead of classifying examples as merely positive or negative, this model could theoretically classify each film, using only its dialog, in zero or more of the genres it is associated with. This would mean that the model would not only be multi-class, like our MNIST example, but multi-label, where each example could have zero or more labels applied to it. Finding out how simple or difficult this multi-label expansion would be was another aspect of interest in this project.\n",
    "\n",
    "Another point of both concern and interest was the dataset itself. Instead of a collection of multiple thousands of examples, the dataset is a collection of only 617 movies, but 304,713 utterances are spread between them. This is a very different distribution than the multiple thousands of reviews with shorter sequences in the IMDb example.\n",
    "\n",
    "So the purpose of this experiment is to see how effective a machine learning model can be in identifying multi-class, multi-label classifications under relatively restricted circumstances. Since screenplays are relatively easy to find after their movies are released, I could also see, if this experiment proves successful enough, that something like this could be used to assist movie classification tools with quickly gathering initial data about their genres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The main technology used for this machine learning model is [Keras](https://www.keras.io/). As a framework for Google's TensorFlow, and the framework utilized in the exercises that formed a basis for this project, it made sense to continue utilizing this instead of another technology. I was not planning on employing an unknown machine learning architecture to this project, so I simply employed Keras' sequential model with Dense layers. Again, the basis for the model was based upon the [IMDb reviews classification exercise](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/3.5-classifying-movie-reviews.ipynb). The only modifications made to it were in the size of the layers, and in the addition of Dropout layers to help fight overfitting.\n",
    "\n",
    "While other avenues were considered, the implementation that ultimately worked for the multi-class, multi-layer implementation was almost identical to the IMDb example. Particularly with the help of [this Stack Overflow answer]()\n",
    "\n",
    "\n",
    "\n",
    "The first \"technology\" worth mentioning is the dataset itself. This dataset is structured by several different files that are linked together by IDs in an SQL-like style. Because of this, and my higher familiarity with SQL than with processing .csv-like files, the dataset I opted to use is an SQLite conversion by Lee Richards, [found on Kaggle here](https://www.kaggle.com/mrlarichards/cornell-movie-dialogs-corpus-sqlite). The database is structured by tables for movies, genres, a linking table between movies and genres, characters, lines, and a conversations table linking characters to lines. For this experiment, only the movies, genres, and lines tables will be used, in order to collect the lines and genres for each movie in a fairly straightforward manner.\n",
    "\n",
    "In terms of raw numbers, the dataset collects 617 movies' dialog lines into what amounts to 304,713 utterances, with the movies being spread across 24 genres. While 617 is a pretty small number of examples for neural network training, the high number of utterances is, what I had hoped, would offset this lack of variety.\n",
    "\n",
    "From there, I planned to build a neural network at first very similar to Chollet's exercise. That is, a sequential neural network with two hidden, densely-connected layers which output 64-bit floats of the probabilities that each movie is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implications"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}