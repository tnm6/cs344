{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "Neural network exercises for CS-344, Professor Keith Vander Linden, Calvin University.\n",
    "\n",
    "Answers by Nathan Meyer (tnm6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1\n",
    "\n",
    "Speculate on whether so-called \"deep\" neural networks will be another bust or really are a breakthrough for the foreseeable future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response\n",
    "\n",
    "My belief is that neural networks are more of a breakthrough than a bust. While technology is destined to change, and so approaches to AI will surely evolve, machine learning and especially deep neural networks shift the paradigm so drastically. More importantly, they are conceptually scalable. This is because the extent to which these AIs can \"learn\" is conceptually infinite, like how bits in binary code can be built into infinitely complex computer systems. A super-intelligence AI could conceivably be built upon sub-models of neural networks dedicated to performing well in specific areas, extending the abilities that the AI can have. Since it is based on the way neurons in our brains learn, this is, at least abstractly, similar to how the human brain can learn complex processes.\n",
    "\n",
    "Take the work that is being done at Google. Translation was considered to be impossible to perform reliably until Google Brain was thrown at it, where performance improved at rates previously unseen before, particularly with complex performance metrics like the \"artistry\" of the language found in poems, etc. Similar work has been done with image classification. Functions like these are specific in and of themselves, but stacked or combined together, they can, perceivably, reach some manner of flexibility in actions performed like the human brain can. The extent to which this could scale is conceptually infinite.\n",
    "\n",
    "The problem is data. These concepts have been around for decades, but only now, with the advent of the internet, have there been datasets large enough to even begin to train neural networks to perform reliably in many areas (like translation). The limit, then, is not so much the conceptual limits of the AI design, but the resources required to \"feed\" the learning models. For now, that training data is highly dependent upon reliable human data entry. Further, training these networks are highly demanding upon hardware. However, while these two issues may limit the breadth of neural network AI for now, improvements in both of them in the future would correlate with greater performance in neural networks. Since hardware improvements are expected (for now), and streamlining or improving data entry may still be possible, I do not see why neural networks will not continue to be the way forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2\n",
    "\n",
    "Hand-compute a single, complete back-propagation cycle. Use the example network from class and compute the updated weight values for the first gradient descent iteration for the XOR example, i.e., [1, 1] â†’ 0. Use the same initial weights we used in the class example but assume the identity function as the activation function (f(x) = x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "1. Fill in random weights (same as class example).\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    &\\begin{bmatrix}\n",
    "    w_{i_1,h_1} & w_{i_1,h_2} \\\\\n",
    "    w_{i_2,h_1} & w_{i_2,h_2}\n",
    "    \\end{bmatrix}\n",
    "    \\leftarrow\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} \\\\\n",
    "    &\\begin{bmatrix}\n",
    "    w_{h_1, o_1} \\\\ \n",
    "    w_{h_2, o_1} \n",
    "    \\end{bmatrix}\n",
    "    \\leftarrow\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\\n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\end{aligned}$\n",
    "\n",
    "2. Compute the output for sample (XOR: `[1, 1]` &rarr; `0`)\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    o_j &= \n",
    "    \\begin{bmatrix}\n",
    "    1 & 1 \\\\ \n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\\n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    1 * 0.11 + 1 * 0.21 & 1 * 0.12 + 1 * 0.08\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.32 & 0.20\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.32 * 0.14 + 0.20 * 0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &= 0.0748\n",
    "    \\end{aligned}\n",
    "    \\\\\n",
    "    $\n",
    "\n",
    "3. Compute the error and delta.\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    L_2Error &= (0 - 0.0748)^2 \\\\\n",
    "    &= 0.0056 \\\\\n",
    "    \\Delta_{o_1} &= (0 - 0.0748) \\\\\n",
    "    &= -0.0748 \\\\\n",
    "    \\end{aligned}$\n",
    "\n",
    "4. Back-propagate through the network, assuming:\n",
    "    $learning\\_rate = 0.05$; \n",
    "    Activation function `f(x) = x`.\n",
    "     \n",
    "    $\\begin{aligned}\n",
    "    \\begin{bmatrix}\n",
    "    w_{h_1, o_1} \\\\ \n",
    "    w_{h_2, o_1}\n",
    "    \\end{bmatrix} &\\leftarrow \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} + 0.05 \\cdot \n",
    "    \\begin{bmatrix}\n",
    "    0.32 \\\\ \n",
    "    0.20\n",
    "    \\end{bmatrix} \\cdot 1.0 \\cdot -0.0748 \\\\\\\\\n",
    "    &= \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * 0.32 * 1.0 * -0.0748 \\\\\n",
    "    0.05 * 0.20 * 1.0 * -0.0748 \n",
    "    \\end{bmatrix} \\\\\n",
    "    &= \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} +\n",
    "    \\begin{bmatrix}\n",
    "    -0.001197 \\\\\n",
    "    -0.000748\n",
    "    \\end{bmatrix} \\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "    0.138803 \\\\ \n",
    "    0.149252\n",
    "    \\end{bmatrix}\n",
    "    \\end{aligned}$\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    \\begin{bmatrix}\n",
    "    w_{i_1,h_1} & w_{i_1,h_2} \\\\ \n",
    "    w_{i_2,h_1} & w_{i_2,h_2}\n",
    "    \\end{bmatrix} &\\leftarrow \n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + 0.05 \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    1 & 1 \\\\ \n",
    "    1 & 1\n",
    "    \\end{bmatrix} \\cdot 1.0 \\odot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 & 0.15 \\\\ \n",
    "    0.14 & 0.15\n",
    "    \\end{bmatrix} \\cdot -0.0748 \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * 1 * 1.0 & 0.05 * 1 * 1.0 \\\\ \n",
    "    0.05 * 1 * 1.0 & 0.05 * 1 * 1.0 \\\\ \n",
    "    \\end{bmatrix} \\odot \n",
    "    \\begin{bmatrix}\n",
    "    0.14 * -0.0748 & 0.15 * -0.0748 \\\\ \n",
    "    0.14 * -0.0748 & 0.15 * -0.0748 \n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 & 0.05 \\\\ \n",
    "    0.05 & 0.05 \n",
    "    \\end{bmatrix} \\odot \n",
    "    \\begin{bmatrix}\n",
    "    -0.010472 & -0.01122 \\\\\n",
    "    -0.010472 & -0.01122\n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * -0.010472 & 0.05 * -0.01122 \\\\\n",
    "    0.05 * -0.010472 & 0.05 * -0.01122\n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} +     \n",
    "    \\begin{bmatrix}\n",
    "    -0.000524 & -0.000561 \\\\\n",
    "    -0.000524 & -0.000561\n",
    "    \\end{bmatrix} \\\\ &= \n",
    "    \\begin{bmatrix}\n",
    "    0.109476 & 0.119439 \\\\\n",
    "    0.209476 & 0.079439\n",
    "    \\end{bmatrix}  \n",
    "    \\end{aligned}$"
   ]
  }
 ]
}