{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "Neural network exercises for CS-344, Professor Keith Vander Linden, Calvin University.\n",
    "\n",
    "Answers by Nathan Meyer (tnm6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1\n",
    "\n",
    "Speculate on whether so-called \"deep\" neural networks will be another bust or really are a breakthrough for the foreseeable future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response\n",
    "\n",
    "My belief is that neural networks are more of a breakthrough than a bust. While technology is destined to change, and so approaches to AI will surely evolve, machine learning and especially deep neural networks shift the paradigm so drastically. More importantly, they are conceptually scalable. This is because the extent to which these AIs can \"learn\" is conceptually infinite, like how bits in binary code can be built into infinitely complex computer systems. A super-intelligence AI could conceivably be built upon sub-models of neural networks dedicated to performing well in specific areas, extending the abilities that the AI can have. Since it is based on the way neurons in our brains learn, this is, at least abstractly, similar to how the human brain can learn complex processes.\n",
    "\n",
    "Take the work that is being done at Google. Translation was considered to be impossible to perform reliably until Google Brain was thrown at it, where performance improved at rates previously unseen before, particularly with complex performance metrics like the \"artistry\" of the language found in poems, etc. Similar work has been done with image classification. Functions like these are specific in and of themselves, but stacked or combined together, they can, perceivably, reach some manner of flexibility in actions performed like the human brain can. The extent to which this could scale is conceptually infinite.\n",
    "\n",
    "The problem is data. These concepts have been around for decades, but only now, with the advent of the internet, have there been datasets large enough to even begin to train neural networks to perform reliably in many areas (like translation). The limit, then, is not so much the conceptual limits of the AI design, but the resources required to \"feed\" the learning models. For now, that training data is highly dependent upon reliable human data entry. Further, training these networks are highly demanding upon hardware. However, while these two issues may limit the breadth of neural network AI for now, improvements in both of them in the future would correlate with greater performance in neural networks. Since hardware improvements are expected (for now), and streamlining or improving data entry may still be possible, I do not see why neural networks will not continue to be the way forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2\n",
    "\n",
    "Hand-compute a single, complete back-propagation cycle. Use the example network from class and compute the updated weight values for the first gradient descent iteration for the XOR example, i.e., [1, 1] → 0. Use the same initial weights we used in the class example but assume the identity function as the activation function (f(x) = x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "1. Fill in random weights (same as class example).\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    &\\begin{bmatrix}\n",
    "    w_{i_1,h_1} & w_{i_1,h_2} \\\\\n",
    "    w_{i_2,h_1} & w_{i_2,h_2}\n",
    "    \\end{bmatrix}\n",
    "    \\leftarrow\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} \\\\\n",
    "    &\\begin{bmatrix}\n",
    "    w_{h_1, o_1} \\\\ \n",
    "    w_{h_2, o_1} \n",
    "    \\end{bmatrix}\n",
    "    \\leftarrow\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\\n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\end{aligned}$\n",
    "\n",
    "2. Compute the output for sample (XOR: `[1, 1]` &rarr; `0`)\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    o_j &= \n",
    "    \\begin{bmatrix}\n",
    "    1 & 1 \\\\ \n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\\n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    1 * 0.11 + 1 * 0.21 & 1 * 0.12 + 1 * 0.08\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.32 & 0.20\n",
    "    \\end{bmatrix}\n",
    "    \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix}\n",
    "    \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.32 * 0.14 + 0.20 * 0.15\n",
    "    \\end{bmatrix}\n",
    "    \\\\ &= 0.0748\n",
    "    \\end{aligned}\n",
    "    \\\\\n",
    "    $\n",
    "\n",
    "3. Compute the error and delta.\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    L_2Error &= (0 - 0.0748)^2 \\\\\n",
    "    &= 0.0056 \\\\\n",
    "    \\Delta_{o_1} &= (0 - 0.0748) \\\\\n",
    "    &= -0.0748 \\\\\n",
    "    \\end{aligned}$\n",
    "\n",
    "4. Back-propagate through the network, assuming:\n",
    "    $learning\\_rate = 0.05$; \n",
    "    Activation function `f(x) = x`.\n",
    "     \n",
    "    $\\begin{aligned}\n",
    "    \\begin{bmatrix}\n",
    "    w_{h_1, o_1} \\\\ \n",
    "    w_{h_2, o_1}\n",
    "    \\end{bmatrix} &\\leftarrow \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} + 0.05 \\cdot \n",
    "    \\begin{bmatrix}\n",
    "    0.32 \\\\ \n",
    "    0.20\n",
    "    \\end{bmatrix} \\cdot 1.0 \\cdot -0.0748 \\\\\\\\\n",
    "    &= \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * 0.32 * 1.0 * -0.0748 \\\\\n",
    "    0.05 * 0.20 * 1.0 * -0.0748 \n",
    "    \\end{bmatrix} \\\\\n",
    "    &= \n",
    "    \\begin{bmatrix}\n",
    "    0.14 \\\\ \n",
    "    0.15 \n",
    "    \\end{bmatrix} +\n",
    "    \\begin{bmatrix}\n",
    "    -0.001197 \\\\\n",
    "    -0.000748\n",
    "    \\end{bmatrix} \\\\\n",
    "    &=\n",
    "    \\begin{bmatrix}\n",
    "    0.138803 \\\\ \n",
    "    0.149252\n",
    "    \\end{bmatrix}\n",
    "    \\end{aligned}$\n",
    "\n",
    "    $\\begin{aligned}\n",
    "    \\begin{bmatrix}\n",
    "    w_{i_1,h_1} & w_{i_1,h_2} \\\\ \n",
    "    w_{i_2,h_1} & w_{i_2,h_2}\n",
    "    \\end{bmatrix} &\\leftarrow \n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + 0.05 \\cdot\n",
    "    \\begin{bmatrix}\n",
    "    1 & 1 \\\\ \n",
    "    1 & 1\n",
    "    \\end{bmatrix} \\cdot 1.0 \\odot\n",
    "    \\begin{bmatrix}\n",
    "    0.14 & 0.15 \\\\ \n",
    "    0.14 & 0.15\n",
    "    \\end{bmatrix} \\cdot -0.0748 \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * 1 * 1.0 & 0.05 * 1 * 1.0 \\\\ \n",
    "    0.05 * 1 * 1.0 & 0.05 * 1 * 1.0 \\\\ \n",
    "    \\end{bmatrix} \\odot \n",
    "    \\begin{bmatrix}\n",
    "    0.14 * -0.0748 & 0.15 * -0.0748 \\\\ \n",
    "    0.14 * -0.0748 & 0.15 * -0.0748 \n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 & 0.05 \\\\ \n",
    "    0.05 & 0.05 \n",
    "    \\end{bmatrix} \\odot \n",
    "    \\begin{bmatrix}\n",
    "    -0.010472 & -0.01122 \\\\\n",
    "    -0.010472 & -0.01122\n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} + \n",
    "    \\begin{bmatrix}\n",
    "    0.05 * -0.010472 & 0.05 * -0.01122 \\\\\n",
    "    0.05 * -0.010472 & 0.05 * -0.01122\n",
    "    \\end{bmatrix} \\\\ &=\n",
    "    \\begin{bmatrix}\n",
    "    0.11 & 0.12 \\\\\n",
    "    0.21 & 0.08\n",
    "    \\end{bmatrix} +     \n",
    "    \\begin{bmatrix}\n",
    "    -0.000524 & -0.000561 \\\\\n",
    "    -0.000524 & -0.000561\n",
    "    \\end{bmatrix} \\\\ &= \n",
    "    \\begin{bmatrix}\n",
    "    0.109476 & 0.119439 \\\\\n",
    "    0.209476 & 0.079439\n",
    "    \\end{bmatrix}  \n",
    "    \\end{aligned}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3\n",
    "\n",
    "Build a Keras-based ConvNet for Keras’s Fashion MNIST dataset (fashion_mnist). Experiment with different network architectures, submit your most performant network, and report the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255\n",
    "\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 60000 samples, validate on 10000 samples\nEpoch 1/10\n15232/60000 [======>.......................] - ETA: 53s - loss: 0.6031 - acc: 0.77"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-58802c324b9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           validation_data=(test_images, test_labels))\n\u001b[0m",
      "\u001b[0;32m~/CS/344/env/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/CS/344/env/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/344/env/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mt_before_callbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_before_callbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mdelta_t_median\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delta_ts_batch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/344/env/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# will be handled by on_epoch_end.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/CS/344/env/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 \u001b[0mnumdigits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m                 \u001b[0mbarstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%%%dd/%d ['\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumdigits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                 \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbarstr\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcurrent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=10,\n",
    "          batch_size=64,\n",
    "          validation_data=(test_images, test_labels))\n",
    "\n",
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size = 128 produces higher accuracy (91% vs ~90.3%)\n",
    "second run resulted in 90.9% though...\n",
    "\n",
    "Reconfigured network with Conv-Relu-Conv-Relu-Pool produced test accuracy of 91.6%"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitenvvenv964bff84c05a45e6adc7a8f890209e2c",
   "display_name": "Python 3.7.6 64-bit ('env': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}