{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitenvvenv964bff84c05a45e6adc7a8f890209e2c",
   "display_name": "Python 3.7.6 64-bit ('env': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homework 2\n",
    "\n",
    "Spam filter and Bayesian network exercises for CS-344, Professor Keith Vander Linden, Calvin University.\n",
    "\n",
    "Completed by Nathan Meyer, 3/10/2020."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.1\n",
    "\n",
    "Implementation of spam filter based on \"A Plan for Spam\" algorithm by Paul Graham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "First, each word in the corpora's messages are counted, starting at 1 for each \"new\" word found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Spam hash table:\n\t{'i': 3, 'am': 2, 'spam': 2, 'do': 1, 'not': 1, 'like': 1, 'that': 1, 'spamiam': 1}\nHam hash table:\n\t{'do': 2, 'i': 2, 'like': 1, 'green': 1, 'eggs': 1, 'and': 1, 'ham': 1}\n"
    }
   ],
   "source": [
    "def hash_corpus(corpus):\n",
    "    '''\n",
    "    Reads a given corpus (list of message lists) and returns a hash table for\n",
    "    the number of occurrences of each word\n",
    "    '''\n",
    "    hashed = {}\n",
    "\n",
    "    for message in corpus:\n",
    "        for word in message:\n",
    "            entry = word.lower()    # ignore case\n",
    "            if entry not in hashed:\n",
    "                hashed[entry] = 1\n",
    "            else:\n",
    "                hashed[entry] += 1\n",
    "\n",
    "    return hashed\n",
    "\n",
    "\n",
    "spam_corpus = [[\"I\", \"am\", \"spam\", \"spam\", \"I\", \"am\"],\n",
    "               [\"I\", \"do\", \"not\", \"like\", \"that\", \"spamiam\"]]\n",
    "ham_corpus = [[\"do\", \"i\", \"like\", \"green\", \"eggs\", \"and\", \"ham\"], [\"i\", \"do\"]]\n",
    "\n",
    "spam = hash_corpus(spam_corpus)\n",
    "ham = hash_corpus(ham_corpus)\n",
    "\n",
    "print(\"Spam hash table:\\n\\t\" + str(spam))\n",
    "print(\"Ham hash table:\\n\\t\" + str(ham))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then begin hashing a table of probabilities for how likely each word is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_prob_table(good, bad, ngood, nbad):\n",
    "    '''Creates a hash table of the probabilities that each word is spam'''\n",
    "    probs = {}\n",
    "    probs.update(good)\n",
    "    probs.update(bad)\n",
    "\n",
    "    for word in probs:\n",
    "        probs[word] = word_spam_prob(word, good, bad, ngood, nbad)\n",
    "\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calls word_spam_prob(), which, based on Graham's algorithm, calculates \"good\" and \"bad\" values for each word based on the good and bad hash tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_spam_prob(word, good, bad, ngood, nbad):\n",
    "    '''Uses Paul Graham's algorithm to determine how likely a word is spam'''\n",
    "    if word in good:\n",
    "        g = 2 * good[word]\n",
    "    else:\n",
    "        g = 0\n",
    "    if word in bad:\n",
    "        b = bad[word]\n",
    "    else:\n",
    "        b = 0\n",
    "\n",
    "    if (g + b) >= 1:\n",
    "        return max(0.01,\n",
    "                   min(0.99,\n",
    "                       min(1.0, b / nbad) / (min(1.0, g / ngood) +\n",
    "                                                 min(1.0, b / nbad))))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the given spam and ham corpora, calling hash_prob_table() on them (with calculated number of messages in each corpus) produces these results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Probability table:\n\t{'do': 0.3333333333333333, 'i': 0.5, 'like': 0.3333333333333333, 'green': 0.01, 'eggs': 0.01, 'and': 0.01, 'ham': 0.01, 'am': 0.99, 'spam': 0.99, 'not': 0.99, 'that': 0.99, 'spamiam': 0.99}\n"
    }
   ],
   "source": [
    "nspam = len(spam_corpus)\n",
    "nham = len(spam_corpus)\n",
    "\n",
    "probabilities = hash_prob_table(ham, spam, nham, nspam)\n",
    "print(\"Probability table:\\n\\t\" + str(probabilities))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can determine the probability that each multi-word message is spam, using the second section of Paul Graham's algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def msg_spam_prob(message, probs):\n",
    "    '''\n",
    "    Determines the probability that an entire message is spam\n",
    "    based on Paul Graham's algorithm\n",
    "    '''\n",
    "    product = 1\n",
    "    complement = 1\n",
    "\n",
    "    for word in message:\n",
    "        word_prob = (probs.get(word.lower()) or 1)\n",
    "        product *= word_prob\n",
    "        complement *= (1 - word_prob)\n",
    "\n",
    "    return product / (product + complement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running msg_spam_prob() on all of the messages in the corpora produces these results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Messages and their probabilities of spam:\n\t['I', 'am', 'spam', 'spam', 'I', 'am']: 0.9999999895897965\n\t['I', 'do', 'not', 'like', 'that', 'spamiam']: 0.999995877576386\n\t['do', 'i', 'like', 'green', 'eggs', 'and', 'ham']: 2.6025508824397714e-09\n\t['i', 'do']: 0.3333333333333333\n"
    }
   ],
   "source": [
    "first_spam_msg = msg_spam_prob(spam_corpus[0], probabilities)\n",
    "second_spam_msg = msg_spam_prob(spam_corpus[1], probabilities)\n",
    "first_ham_msg = msg_spam_prob(ham_corpus[0], probabilities)\n",
    "second_ham_msg = msg_spam_prob(ham_corpus[1], probabilities)\n",
    "\n",
    "print(\"Messages and their probabilities of spam:\")\n",
    "print(\"\\t\" + str(spam_corpus[0]) + \": \" + str(first_spam_msg))\n",
    "print(\"\\t\" + str(spam_corpus[1]) + \": \" + str(second_spam_msg))\n",
    "print(\"\\t\" + str(ham_corpus[0]) + \": \" + str(first_ham_msg))\n",
    "print(\"\\t\" + str(ham_corpus[1]) + \": \" + str(second_ham_msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What makes it Bayesian?\n",
    "\n",
    "In regards to the probability of a message being spam, it opts to only evaluate the probability based upon the words that are present within the message. The probability that 'do' is spam or not is not relevant to the \"I am spam, spam I am\" message, nor any other message which does not contain 'do', so it is not considered. In this way, a Bayesian network of sorts is formed for each message, gathering probabilities given that certain words appear in the message. Within this approach, words missing from the message are irrelevant, so those probabilities are not even considered, like a Bayesian network."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.2"
   ]
  }
 ]
}