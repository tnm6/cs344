Exercise 9.2 of Lab 8
CS-344, Professor Vander Linden, Calvin University
Answers by Nathan Meyer (tnm6)

a.  Why are we regularizing with respect to sparsity?
  High sparsity often leads to vectors with many dimensions, so regularizing
with respect to sparsity can help to "zero out" certain features, meaning they
are essentially removed. This saves memory and can also reduce noise. This is
particularly possible in L1 Regularization which encourages coefficients to be
0 where possible.

b.  How does L1 regularization increase sparsity?
  As referred to in a., L1 Regularization encourages coefficients to be 0. This
can lead to elimination of certain features which saves memory and can reduce
input noise as well. Since sparsity is the number of zero'd elements divided by
the total number of elements, regularization will inherently increase sparsity
as it pushes more and more values to 0.

c.  Task 1: Here, just report the best log loss value / model size you can get
    and what gamma value you used to get them.
  LogLoss: 0.25
  Model Size: 574
  Gamma: 0.75