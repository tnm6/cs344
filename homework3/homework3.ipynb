{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "Regression exercises for CS-344, Professor Keith Vander Linden, Calvin University.\n",
    "\n",
    "Answers/solutions by Nathan Meyer (tnm6)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "\n",
    "Computation of the information gain provided by using the price attribute as the root of the decision tree in the restaurant example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "```\n",
    "Gain(Price) = Entropy(WillWait?) - Remainder(Price)\n",
    "    Entropy(WillWait?) = 1.0\n",
    "    Remainder(Price) = 3/12*Entropy($$$) + 2/12*Entropy($$) + 7/12*Entropy($)\n",
    "        Entropy($$$) = -(1/3*lg(1/3) + 2/3*lg(2/3)) ~= 0.9183\n",
    "        Entropy($$)  = -(2/2*lg(2/2) + 0/2*lg(0/2)) ~= 0\n",
    "        Entropy($)   = -(3/7*lg(3/7) + 4/7*lg(4/7)) ~= 0.9852\n",
    "    = (3/12)*0.9183 + (2/12)*0 + (7/12)*0.9852 ~= 0.8043\n",
    "= 1.0 - 0.8043 ~= 0.1957\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to Type and Patrons\n",
    "\n",
    "Based on the information gain calculation for Price (0.1957 bits), it is greater gain than for Type (0 bits) but less gain than Patrons (0.541 bits)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2\n",
    "\n",
    "In class, we attempted to create by hand a neural network that computes the XOR function. If this was possible, see if you can simplify the network we built. Consider relaxing the conventions of densely-connected, sequential layers. If it was not possible, give a full explanation why it canâ€™t be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was it possible?\n",
    "\n",
    "As discussed in class, the XOR function was not found to be possible in a perceptron layout with one input layer and one output layer. However, if we add a second, hidden layer (two nodes wide with a bias input), the XOR function can be reliably achieved through back propagation (this is a neural network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Network\n",
    "\n",
    "If we can relax the conventions of dense connectivity and layers being strictly sequential, the network can be simplified further. Rather than the model with the hidden layer also having two nodes like the input layer, the hidden layer could have one node, which receives input from both nodes at the input layer. At the same time, the input layer feeds directly into the single output layer, in addition to the hidden layer. The hidden layer also feeds into the output layer as normal.\n",
    "\n",
    "As for the weights, for one example configuration, the input nodes can have weights of 1. For the hidden layer, if the inputs amount to greater than 1.5 (or 1.9 or anything just under 2), the hidden node will add a weight of -2 to the output layer. The output layer will only output true (or 1) if the inputs and weights it receives amount to greater than 0.5 (or, again, 0.9 or anything below 1).\n",
    "\n",
    "In this configuration, the output will layer will only output true if *one* of the input layers is 1 and the other is 0. If both are 1, the hidden layer will \"cancel out\" their cumulative inputs (a total of 2) by adding a weight of -2, which is under the threshold required for the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ]
}