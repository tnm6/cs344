{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitenvvenv964bff84c05a45e6adc7a8f890209e2c",
   "display_name": "Python 3.7.6 64-bit ('env': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "Regression exercises for CS-344, Professor Keith Vander Linden, Calvin University.\n",
    "\n",
    "Answers/solutions by Nathan Meyer (tnm6)."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1\n",
    "\n",
    "Computation of the information gain provided by using the price attribute as the root of the decision tree in the restaurant example."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation\n",
    "\n",
    "```\n",
    "Gain(Price) = Entropy(WillWait?) - Remainder(Price)\n",
    "    Entropy(WillWait?) = 1.0\n",
    "    Remainder(Price) = 3/12*Entropy($$$) + 2/12*Entropy($$) + 7/12*Entropy($)\n",
    "        Entropy($$$) = -(1/3*lg(1/3) + 2/3*lg(2/3)) ~= 0.9183\n",
    "        Entropy($$)  = -(2/2*lg(2/2) + 0/2*lg(0/2)) ~= 0\n",
    "        Entropy($)   = -(3/7*lg(3/7) + 4/7*lg(4/7)) ~= 0.9852\n",
    "    = (3/12)*0.9183 + (2/12)*0 + (7/12)*0.9852 ~= 0.8043\n",
    "= 1.0 - 0.8043 ~= 0.1957\n",
    "```\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to Type and Patrons\n",
    "\n",
    "Based on the information gain calculation for Price (0.1957 bits), it is greater gain than for Type (0 bits) but less gain than Patrons (0.541 bits)."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2\n",
    "\n",
    "In class, we attempted to create by hand a neural network that computes the XOR function. If this was possible, see if you can simplify the network we built. Consider relaxing the conventions of densely-connected, sequential layers. If it was not possible, give a full explanation why it canâ€™t be done."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Was it possible?\n",
    "\n",
    "As discussed in class, the XOR function was not found to be possible in a perceptron layout with one input layer and one output layer. However, if we add a second, hidden layer (two nodes wide with a bias input), the XOR function can be reliably achieved through back propagation (this is a neural network)."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified Network\n",
    "\n",
    "If we can relax the conventions of dense connectivity and layers being strictly sequential, the network can be simplified further. Rather than the model with the hidden layer also having two nodes like the input layer, the hidden layer could have one node, which receives input from both nodes at the input layer. At the same time, the input layer feeds directly into the single output layer, in addition to the hidden layer. The hidden layer also feeds into the output layer as normal.\n",
    "\n",
    "As for the weights, for one example configuration, the input nodes can have weights of 1. For the hidden layer, if the inputs amount to greater than 1.5 (or 1.9 or anything just under 2), the hidden node will add a weight of -2 to the output layer. The output layer will only output true (or 1) if the inputs and weights it receives amount to greater than 0.5 (or, again, 0.9 or anything below 1).\n",
    "\n",
    "In this configuration, the output will layer will only output true if *one* of the input layers is 1 and the other is 0. If both are 1, the hidden layer will \"cancel out\" their cumulative inputs (a total of 2) by adding a weight of -2, which is under the threshold required for the output.\n",
    "\n",
    "This setup is not possible with a network of sequential layers, but with that restriction lifted, it would be."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3\n",
    "\n",
    "Use Python/NumPy/Pandas/Keras to load and manipulate the Boston housing Dataset."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Dimensions\n",
    "\n",
    "Code to compute and print the values of the dimensions of the data structures.\n",
    "\n",
    "(Opted to base code on downloading a .csv version of the data, since the exercise is open-ended.)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Dimensions: (506, 14)\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV version of Boston Housing Dataset found via quick Google search\n",
    "boston_housing_dataframe = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv\")\n",
    "\n",
    "print(\"Dimensions: \" + str(boston_housing_dataframe.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Construct Datasets\n",
    "\n",
    "Construct a suitable testing set, training set, and validation set for this data. Submit code to create these datasets but do not include the datasets themselves."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def process_features(boston_housing_dataframe):\n",
    "    return boston_housing_dataframe[\n",
    "        [\"crim\",\n",
    "         \"zn\", \n",
    "         \"indus\", \n",
    "         \"chas\", \n",
    "         \"nox\", \n",
    "         \"rm\", \n",
    "         \"age\", \n",
    "         \"dis\", \n",
    "         \"rad\", \n",
    "         \"tax\", \n",
    "         \"ptratio\", \n",
    "         \"b\", \n",
    "         \"lstat\"]]\n",
    "\n",
    "def process_targets(boston_housing_dataframe):\n",
    "    return boston_housing_dataframe[\"medv\"]\n",
    "\n",
    "# First shuffle data\n",
    "boston_housing_dataframe = boston_housing_dataframe.reindex(\n",
    "    np.random.permutation(boston_housing_dataframe.index))\n",
    "\n",
    "# Process training and testing datasets\n",
    "training_examples = process_features(boston_housing_dataframe.head(404))\n",
    "training_targets = process_targets(boston_housing_dataframe.head(404))\n",
    "testing_examples = process_features(boston_housing_dataframe.tail(102))\n",
    "testing_targets = process_targets(boston_housing_dataframe.tail(102))\n",
    "\n",
    "# Further split training dataset into training and validation datasets\n",
    "val_examples = training_examples.tail(102)\n",
    "val_targets = training_targets.tail(102)\n",
    "training_examples = training_examples.head(302)\n",
    "training_targets = training_targets.head(302)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Synthetic Feature\n",
    "\n",
    "Create one new synthetic feature that could be useful for machine learning in this domain."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_housing_dataframe[\"crime_by_class\"] =\n",
    "    boston_housing_dataframe[\"crim\"] / boston_housing_dataframe[\"lstat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This synthetic feature can evaluate the rate of crime per capita per the percentage of lower status population in the town. This could be a useful ratio since higher crime and lower income might affect the home median value negatively, and the opposite could be true. Although this could be perpetuating a stereotype, the synthetic feature might help the effectiveness of the learning model. However, since ML models do carry the biases of their designers with them, it would need to be thoroughly evaluated whether this feature purely enhances the model's accuracy, or skews it toward a harmful bias."
   ]
  }
 ]
}